#!/bin/bash

# Setup script for News Pipeline Project on WSL Ubuntu

echo "ðŸš€ Setting up News Pipeline Project..."

# Create src/db_utils.py
cat > src/db_utils.py << 'EOF'
import sqlite3
import os
from config import SQLITE_DB_PATH


def init_database():
    """Initialize SQLite database with required tables"""
    os.makedirs(os.path.dirname(SQLITE_DB_PATH), exist_ok=True)
    
    conn = sqlite3.connect(SQLITE_DB_PATH)
    cursor = conn.cursor()
    
    # Create news_events table (cleaned data)
    cursor.execute('''
    CREATE TABLE IF NOT EXISTS news_events (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        article_id TEXT UNIQUE,
        title TEXT NOT NULL,
        description TEXT,
        content TEXT,
        link TEXT,
        source_id TEXT,
        source_name TEXT,
        category TEXT,
        country TEXT,
        language TEXT,
        pubDate TEXT,
        ingested_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        cleaned_at TIMESTAMP
    )
    ''')
    
    # Create daily_summary table (analytics)
    cursor.execute('''
    CREATE TABLE IF NOT EXISTS daily_summary (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        summary_date DATE NOT NULL,
        total_articles INTEGER,
        unique_sources INTEGER,
        top_category TEXT,
        top_country TEXT,
        avg_title_length REAL,
        articles_by_category TEXT,
        articles_by_source TEXT,
        articles_by_country TEXT,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        UNIQUE(summary_date)
    )
    ''')
    
    conn.commit()
    conn.close()
    print(f"Database initialized at {SQLITE_DB_PATH}")


def get_connection():
    """Get database connection"""
    return sqlite3.connect(SQLITE_DB_PATH)


if __name__ == "__main__":
    init_database()
EOF

# Create src/job1_producer.py
cat > src/job1_producer.py << 'EOF'
import time
import json
import requests
from kafka import KafkaProducer
from datetime import datetime
from config import (
    NEWSDATA_BASE_URL,
    NEWSDATA_PARAMS,
    KAFKA_BOOTSTRAP_SERVERS,
    KAFKA_TOPIC_RAW,
    FETCH_INTERVAL_SECONDS,
    MAX_ITERATIONS
)


class NewsProducer:
    def __init__(self):
        self.producer = KafkaProducer(
            bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )
        self.session = requests.Session()
    
    def fetch_news(self):
        """Fetch news from NewsData.io API"""
        try:
            response = self.session.get(NEWSDATA_BASE_URL, params=NEWSDATA_PARAMS, timeout=10)
            response.raise_for_status()
            data = response.json()
            
            if data.get('status') == 'success':
                return data.get('results', [])
            else:
                print(f"API Error: {data}")
                return []
        except Exception as e:
            print(f"Error fetching news: {e}")
            return []
    
    def send_to_kafka(self, articles):
        """Send articles to Kafka topic"""
        sent_count = 0
        for article in articles:
            try:
                message = {
                    'data': article,
                    'ingested_at': datetime.utcnow().isoformat(),
                    'source': 'newsdata.io'
                }
                
                self.producer.send(KAFKA_TOPIC_RAW, value=message)
                sent_count += 1
            except Exception as e:
                print(f"Error sending to Kafka: {e}")
        
        self.producer.flush()
        return sent_count
    
    def run(self, iterations=None):
        """Run continuous ingestion"""
        iterations = iterations or MAX_ITERATIONS
        print(f"Starting NewsProducer - will run for {iterations} iterations")
        
        for i in range(iterations):
            print(f"\n[Iteration {i+1}/{iterations}] Fetching news at {datetime.utcnow()}")
            
            articles = self.fetch_news()
            if articles:
                sent = self.send_to_kafka(articles)
                print(f"âœ“ Fetched {len(articles)} articles, sent {sent} to Kafka")
            else:
                print("âœ— No articles fetched")
            
            if i < iterations - 1:
                print(f"Sleeping for {FETCH_INTERVAL_SECONDS} seconds...")
                time.sleep(FETCH_INTERVAL_SECONDS)
        
        print("\nâœ“ NewsProducer completed successfully")
        self.producer.close()


def run_producer():
    """Entry point for Airflow"""
    producer = NewsProducer()
    producer.run(iterations=30)


if __name__ == "__main__":
    run_producer()
EOF

# Create src/job2_cleaner.py
cat > src/job2_cleaner.py << 'EOF'
import json
import sqlite3
from datetime import datetime
from kafka import KafkaConsumer
from config import (
    KAFKA_BOOTSTRAP_SERVERS,
    KAFKA_TOPIC_RAW,
    SQLITE_DB_PATH,
    BATCH_SIZE
)


class NewsDataCleaner:
    def __init__(self):
        self.conn = sqlite3.connect(SQLITE_DB_PATH)
        self.cursor = self.conn.cursor()
    
    def clean_article(self, raw_article):
        """Clean and validate article data"""
        data = raw_article.get('data', {})
        
        article_id = data.get('article_id', '')
        title = data.get('title', '').strip()
        description = data.get('description', '').strip() if data.get('description') else None
        content = data.get('content', '').strip() if data.get('content') else None
        link = data.get('link', '').strip()
        
        source_id = data.get('source_id', '')
        source_name = data.get('source_name', 'Unknown')
        
        category = ','.join(data.get('category', [])) if isinstance(data.get('category'), list) else data.get('category', 'uncategorized')
        country = ','.join(data.get('country', [])) if isinstance(data.get('country'), list) else data.get('country', 'unknown')
        language = data.get('language', 'en')
        pub_date = data.get('pubDate', '')
        
        if not article_id or not title:
            return None
        
        if len(title) < 10:
            return None
        
        title = ' '.join(title.split())
        if description:
            description = ' '.join(description.split())
        if content:
            content = ' '.join(content.split())
        
        return {
            'article_id': article_id,
            'title': title,
            'description': description,
            'content': content,
            'link': link,
            'source_id': source_id,
            'source_name': source_name,
            'category': category.lower(),
            'country': country.lower(),
            'language': language.lower(),
            'pubDate': pub_date,
            'cleaned_at': datetime.utcnow().isoformat()
        }
    
    def insert_article(self, article):
        """Insert cleaned article into database"""
        try:
            self.cursor.execute('''
            INSERT OR IGNORE INTO news_events 
            (article_id, title, description, content, link, source_id, source_name, 
             category, country, language, pubDate, cleaned_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                article['article_id'],
                article['title'],
                article['description'],
                article['content'],
                article['link'],
                article['source_id'],
                article['source_name'],
                article['category'],
                article['country'],
                article['language'],
                article['pubDate'],
                article['cleaned_at']
            ))
            return True
        except sqlite3.IntegrityError:
            return False
        except Exception as e:
            print(f"Error inserting article: {e}")
            return False
    
    def process_batch(self):
        """Read from Kafka, clean, and store in SQLite"""
        consumer = KafkaConsumer(
            KAFKA_TOPIC_RAW,
            bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
            auto_offset_reset='earliest',
            enable_auto_commit=True,
            group_id='news_cleaner_group',
            value_deserializer=lambda m: json.loads(m.decode('utf-8')),
            consumer_timeout_ms=30000
        )
        
        processed = 0
        inserted = 0
        skipped = 0
        
        print(f"Starting batch processing from Kafka topic '{KAFKA_TOPIC_RAW}'")
        
        for message in consumer:
            try:
                raw_article = message.value
                cleaned = self.clean_article(raw_article)
                
                if cleaned:
                    if self.insert_article(cleaned):
                        inserted += 1
                    else:
                        skipped += 1
                else:
                    skipped += 1
                
                processed += 1
                
                if processed >= BATCH_SIZE:
                    break
                    
            except Exception as e:
                print(f"Error processing message: {e}")
                skipped += 1
        
        self.conn.commit()
        consumer.close()
        
        print(f"âœ“ Batch complete: {processed} processed, {inserted} inserted, {skipped} skipped")
        return processed, inserted, skipped
    
    def close(self):
        self.conn.close()


def run_cleaner():
    """Entry point for Airflow"""
    cleaner = NewsDataCleaner()
    try:
        cleaner.process_batch()
    finally:
        cleaner.close()


if __name__ == "__main__":
    run_cleaner()
EOF

# Create src/job3_analytics.py
cat > src/job3_analytics.py << 'EOF'
import json
import sqlite3
from datetime import datetime, timedelta
from collections import Counter
from config import SQLITE_DB_PATH, ANALYTICS_LOOKBACK_DAYS


class NewsAnalytics:
    def __init__(self):
        self.conn = sqlite3.connect(SQLITE_DB_PATH)
        self.cursor = self.conn.cursor()
    
    def get_recent_articles(self):
        """Fetch articles from the last 24 hours"""
        yesterday = (datetime.utcnow() - timedelta(days=1)).isoformat()
        
        self.cursor.execute('''
        SELECT article_id, title, description, source_name, category, country, language, pubDate
        FROM news_events
        WHERE cleaned_at >= ?
        ''', (yesterday,))
        
        columns = ['article_id', 'title', 'description', 'source_name', 'category', 'country', 'language', 'pubDate']
        articles = [dict(zip(columns, row)) for row in self.cursor.fetchall()]
        
        return articles
    
    def compute_analytics(self, articles):
        """Compute aggregated metrics"""
        if not articles:
            return None
        
        total_articles = len(articles)
        unique_sources = len(set(a['source_name'] for a in articles if a['source_name']))
        
        categories = [a['category'] for a in articles if a['category']]
        category_counts = Counter(categories)
        top_category = category_counts.most_common(1)[0][0] if category_counts else 'unknown'
        
        countries = []
        for a in articles:
            if a['country']:
                countries.extend(a['country'].split(','))
        country_counts = Counter(countries)
        top_country = country_counts.most_common(1)[0][0] if country_counts else 'unknown'
        
        source_counts = Counter(a['source_name'] for a in articles if a['source_name'])
        
        title_lengths = [len(a['title']) for a in articles if a['title']]
        avg_title_length = sum(title_lengths) / len(title_lengths) if title_lengths else 0
        
        analytics = {
            'summary_date': datetime.utcnow().date().isoformat(),
            'total_articles': total_articles,
            'unique_sources': unique_sources,
            'top_category': top_category,
            'top_country': top_country,
            'avg_title_length': round(avg_title_length, 2),
            'articles_by_category': json.dumps(dict(category_counts.most_common(10))),
            'articles_by_source': json.dumps(dict(source_counts.most_common(10))),
            'articles_by_country': json.dumps(dict(country_counts.most_common(10)))
        }
        
        return analytics
    
    def save_summary(self, analytics):
        """Save analytics to daily_summary table"""
        try:
            self.cursor.execute('''
            INSERT OR REPLACE INTO daily_summary 
            (summary_date, total_articles, unique_sources, top_category, top_country,
             avg_title_length, articles_by_category, articles_by_source, articles_by_country)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                analytics['summary_date'],
                analytics['total_articles'],
                analytics['unique_sources'],
                analytics['top_category'],
                analytics['top_country'],
                analytics['avg_title_length'],
                analytics['articles_by_category'],
                analytics['articles_by_source'],
                analytics['articles_by_country']
            ))
            self.conn.commit()
            return True
        except Exception as e:
            print(f"Error saving summary: {e}")
            return False
    
    def run_analytics(self):
        """Run daily analytics job"""
        print(f"Starting daily analytics for {datetime.utcnow().date()}")
        
        articles = self.get_recent_articles()
        print(f"Found {len(articles)} articles from last 24 hours")
        
        if not articles:
            print("No articles to analyze")
            return
        
        analytics = self.compute_analytics(articles)
        
        if analytics:
            print("\n=== Daily Summary ===")
            print(f"Date: {analytics['summary_date']}")
            print(f"Total Articles: {analytics['total_articles']}")
            print(f"Unique Sources: {analytics['unique_sources']}")
            print(f"Top Category: {analytics['top_category']}")
            print(f"Top Country: {analytics['top_country']}")
            print(f"Avg Title Length: {analytics['avg_title_length']}")
            
            if self.save_summary(analytics):
                print("\nâœ“ Analytics saved to daily_summary table")
            else:
                print("\nâœ— Failed to save analytics")
        
    def close(self):
        self.conn.close()


def run_analytics():
    """Entry point for Airflow"""
    analytics = NewsAnalytics()
    try:
        analytics.run_analytics()
    finally:
        analytics.close()


if __name__ == "__main__":
    run_analytics()
EOF

# Create Airflow DAGs
mkdir -p airflow/dags

cat > airflow/dags/job1_ingestion_dag.py << 'EOF'
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import sys
import os

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

from src.job1_producer import run_producer

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'job1_news_ingestion',
    default_args=default_args,
    description='Continuous news ingestion from NewsData.io to Kafka',
    schedule_interval='@hourly',
    catchup=False,
    tags=['news', 'ingestion', 'kafka'],
)

ingestion_task = PythonOperator(
    task_id='fetch_and_produce_news',
    python_callable=run_producer,
    dag=dag,
)

ingestion_task
EOF

cat > airflow/dags/job2_clean_store_dag.py << 'EOF'
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import sys
import os

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

from src.job2_cleaner import run_cleaner

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'job2_news_cleaning',
    default_args=default_args,
    description='Hourly batch: Read Kafka, clean data, store in SQLite',
    schedule_interval='@hourly',
    catchup=False,
    tags=['news', 'cleaning', 'sqlite'],
)

cleaning_task = PythonOperator(
    task_id='clean_and_store_news',
    python_callable=run_cleaner,
    dag=dag,
)

cleaning_task
EOF

cat > airflow/dags/job3_daily_summary_dag.py << 'EOF'
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import sys
import os

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

from src.job3_analytics import run_analytics

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=10),
}

dag = DAG(
    'job3_daily_analytics',
    default_args=default_args,
    description='Daily analytics: Compute summary from SQLite',
    schedule_interval='@daily',
    catchup=False,
    tags=['news', 'analytics', 'summary'],
)

analytics_task = PythonOperator(
    task_id='compute_daily_summary',
    python_callable=run_analytics,
    dag=dag,
)

analytics_task
EOF

echo "âœ… All Python files created successfully!"
echo ""
echo "Next steps:"
echo "1. Edit config.py and add your NewsData.io API key"
echo "2. Run: chmod +x setup_project.sh"
echo "3. Initialize database: python3 src/db_utils.py"#!/bin/bash

# Setup script for News Pipeline Project on WSL Ubuntu

echo "ðŸš€ Setting up News Pipeline Project..."

# Create src/db_utils.py
cat > src/db_utils.py << 'EOF'
import sqlite3
import os
from config import SQLITE_DB_PATH


def init_database():
    """Initialize SQLite database with required tables"""
    os.makedirs(os.path.dirname(SQLITE_DB_PATH), exist_ok=True)
    
    conn = sqlite3.connect(SQLITE_DB_PATH)
    cursor = conn.cursor()
    
    # Create news_events table (cleaned data)
    cursor.execute('''
    CREATE TABLE IF NOT EXISTS news_events (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        article_id TEXT UNIQUE,
        title TEXT NOT NULL,
        description TEXT,
        content TEXT,
        link TEXT,
        source_id TEXT,
        source_name TEXT,
        category TEXT,
        country TEXT,
        language TEXT,
        pubDate TEXT,
        ingested_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        cleaned_at TIMESTAMP
    )
    ''')
    
    # Create daily_summary table (analytics)
    cursor.execute('''
    CREATE TABLE IF NOT EXISTS daily_summary (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        summary_date DATE NOT NULL,
        total_articles INTEGER,
        unique_sources INTEGER,
        top_category TEXT,
        top_country TEXT,
        avg_title_length REAL,
        articles_by_category TEXT,
        articles_by_source TEXT,
        articles_by_country TEXT,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        UNIQUE(summary_date)
    )
    ''')
    
    conn.commit()
    conn.close()
    print(f"Database initialized at {SQLITE_DB_PATH}")


def get_connection():
    """Get database connection"""
    return sqlite3.connect(SQLITE_DB_PATH)


if __name__ == "__main__":
    init_database()
EOF

# Create src/job1_producer.py
cat > src/job1_producer.py << 'EOF'
import time
import json
import requests
from kafka import KafkaProducer
from datetime import datetime
from config import (
    NEWSDATA_BASE_URL,
    NEWSDATA_PARAMS,
    KAFKA_BOOTSTRAP_SERVERS,
    KAFKA_TOPIC_RAW,
    FETCH_INTERVAL_SECONDS,
    MAX_ITERATIONS
)


class NewsProducer:
    def __init__(self):
        self.producer = KafkaProducer(
            bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )
        self.session = requests.Session()
    
    def fetch_news(self):
        """Fetch news from NewsData.io API"""
        try:
            response = self.session.get(NEWSDATA_BASE_URL, params=NEWSDATA_PARAMS, timeout=10)
            response.raise_for_status()
            data = response.json()
            
            if data.get('status') == 'success':
                return data.get('results', [])
            else:
                print(f"API Error: {data}")
                return []
        except Exception as e:
            print(f"Error fetching news: {e}")
            return []
    
    def send_to_kafka(self, articles):
        """Send articles to Kafka topic"""
        sent_count = 0
        for article in articles:
            try:
                message = {
                    'data': article,
                    'ingested_at': datetime.utcnow().isoformat(),
                    'source': 'newsdata.io'
                }
                
                self.producer.send(KAFKA_TOPIC_RAW, value=message)
                sent_count += 1
            except Exception as e:
                print(f"Error sending to Kafka: {e}")
        
        self.producer.flush()
        return sent_count
    
    def run(self, iterations=None):
        """Run continuous ingestion"""
        iterations = iterations or MAX_ITERATIONS
        print(f"Starting NewsProducer - will run for {iterations} iterations")
        
        for i in range(iterations):
            print(f"\n[Iteration {i+1}/{iterations}] Fetching news at {datetime.utcnow()}")
            
            articles = self.fetch_news()
            if articles:
                sent = self.send_to_kafka(articles)
                print(f"âœ“ Fetched {len(articles)} articles, sent {sent} to Kafka")
            else:
                print("âœ— No articles fetched")
            
            if i < iterations - 1:
                print(f"Sleeping for {FETCH_INTERVAL_SECONDS} seconds...")
                time.sleep(FETCH_INTERVAL_SECONDS)
        
        print("\nâœ“ NewsProducer completed successfully")
        self.producer.close()


def run_producer():
    """Entry point for Airflow"""
    producer = NewsProducer()
    producer.run(iterations=30)


if __name__ == "__main__":
    run_producer()
EOF

# Create src/job2_cleaner.py
cat > src/job2_cleaner.py << 'EOF'
import json
import sqlite3
from datetime import datetime
from kafka import KafkaConsumer
from config import (
    KAFKA_BOOTSTRAP_SERVERS,
    KAFKA_TOPIC_RAW,
    SQLITE_DB_PATH,
    BATCH_SIZE
)


class NewsDataCleaner:
    def __init__(self):
        self.conn = sqlite3.connect(SQLITE_DB_PATH)
        self.cursor = self.conn.cursor()
    
    def clean_article(self, raw_article):
        """Clean and validate article data"""
        data = raw_article.get('data', {})
        
        article_id = data.get('article_id', '')
        title = data.get('title', '').strip()
        description = data.get('description', '').strip() if data.get('description') else None
        content = data.get('content', '').strip() if data.get('content') else None
        link = data.get('link', '').strip()
        
        source_id = data.get('source_id', '')
        source_name = data.get('source_name', 'Unknown')
        
        category = ','.join(data.get('category', [])) if isinstance(data.get('category'), list) else data.get('category', 'uncategorized')
        country = ','.join(data.get('country', [])) if isinstance(data.get('country'), list) else data.get('country', 'unknown')
        language = data.get('language', 'en')
        pub_date = data.get('pubDate', '')
        
        if not article_id or not title:
            return None
        
        if len(title) < 10:
            return None
        
        title = ' '.join(title.split())
        if description:
            description = ' '.join(description.split())
        if content:
            content = ' '.join(content.split())
        
        return {
            'article_id': article_id,
            'title': title,
            'description': description,
            'content': content,
            'link': link,
            'source_id': source_id,
            'source_name': source_name,
            'category': category.lower(),
            'country': country.lower(),
            'language': language.lower(),
            'pubDate': pub_date,
            'cleaned_at': datetime.utcnow().isoformat()
        }
    
    def insert_article(self, article):
        """Insert cleaned article into database"""
        try:
            self.cursor.execute('''
            INSERT OR IGNORE INTO news_events 
            (article_id, title, description, content, link, source_id, source_name, 
             category, country, language, pubDate, cleaned_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                article['article_id'],
                article['title'],
                article['description'],
                article['content'],
                article['link'],
                article['source_id'],
                article['source_name'],
                article['category'],
                article['country'],
                article['language'],
                article['pubDate'],
                article['cleaned_at']
            ))
            return True
        except sqlite3.IntegrityError:
            return False
        except Exception as e:
            print(f"Error inserting article: {e}")
            return False
    
    def process_batch(self):
        """Read from Kafka, clean, and store in SQLite"""
        consumer = KafkaConsumer(
            KAFKA_TOPIC_RAW,
            bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
            auto_offset_reset='earliest',
            enable_auto_commit=True,
            group_id='news_cleaner_group',
            value_deserializer=lambda m: json.loads(m.decode('utf-8')),
            consumer_timeout_ms=30000
        )
        
        processed = 0
        inserted = 0
        skipped = 0
        
        print(f"Starting batch processing from Kafka topic '{KAFKA_TOPIC_RAW}'")
        
        for message in consumer:
            try:
                raw_article = message.value
                cleaned = self.clean_article(raw_article)
                
                if cleaned:
                    if self.insert_article(cleaned):
                        inserted += 1
                    else:
                        skipped += 1
                else:
                    skipped += 1
                
                processed += 1
                
                if processed >= BATCH_SIZE:
                    break
                    
            except Exception as e:
                print(f"Error processing message: {e}")
                skipped += 1
        
        self.conn.commit()
        consumer.close()
        
        print(f"âœ“ Batch complete: {processed} processed, {inserted} inserted, {skipped} skipped")
        return processed, inserted, skipped
    
    def close(self):
        self.conn.close()


def run_cleaner():
    """Entry point for Airflow"""
    cleaner = NewsDataCleaner()
    try:
        cleaner.process_batch()
    finally:
        cleaner.close()


if __name__ == "__main__":
    run_cleaner()
EOF

# Create src/job3_analytics.py
cat > src/job3_analytics.py << 'EOF'
import json
import sqlite3
from datetime import datetime, timedelta
from collections import Counter
from config import SQLITE_DB_PATH, ANALYTICS_LOOKBACK_DAYS


class NewsAnalytics:
    def __init__(self):
        self.conn = sqlite3.connect(SQLITE_DB_PATH)
        self.cursor = self.conn.cursor()
    
    def get_recent_articles(self):
        """Fetch articles from the last 24 hours"""
        yesterday = (datetime.utcnow() - timedelta(days=1)).isoformat()
        
        self.cursor.execute('''
        SELECT article_id, title, description, source_name, category, country, language, pubDate
        FROM news_events
        WHERE cleaned_at >= ?
        ''', (yesterday,))
        
        columns = ['article_id', 'title', 'description', 'source_name', 'category', 'country', 'language', 'pubDate']
        articles = [dict(zip(columns, row)) for row in self.cursor.fetchall()]
        
        return articles
    
    def compute_analytics(self, articles):
        """Compute aggregated metrics"""
        if not articles:
            return None
        
        total_articles = len(articles)
        unique_sources = len(set(a['source_name'] for a in articles if a['source_name']))
        
        categories = [a['category'] for a in articles if a['category']]
        category_counts = Counter(categories)
        top_category = category_counts.most_common(1)[0][0] if category_counts else 'unknown'
        
        countries = []
        for a in articles:
            if a['country']:
                countries.extend(a['country'].split(','))
        country_counts = Counter(countries)
        top_country = country_counts.most_common(1)[0][0] if country_counts else 'unknown'
        
        source_counts = Counter(a['source_name'] for a in articles if a['source_name'])
        
        title_lengths = [len(a['title']) for a in articles if a['title']]
        avg_title_length = sum(title_lengths) / len(title_lengths) if title_lengths else 0
        
        analytics = {
            'summary_date': datetime.utcnow().date().isoformat(),
            'total_articles': total_articles,
            'unique_sources': unique_sources,
            'top_category': top_category,
            'top_country': top_country,
            'avg_title_length': round(avg_title_length, 2),
            'articles_by_category': json.dumps(dict(category_counts.most_common(10))),
            'articles_by_source': json.dumps(dict(source_counts.most_common(10))),
            'articles_by_country': json.dumps(dict(country_counts.most_common(10)))
        }
        
        return analytics
    
    def save_summary(self, analytics):
        """Save analytics to daily_summary table"""
        try:
            self.cursor.execute('''
            INSERT OR REPLACE INTO daily_summary 
            (summary_date, total_articles, unique_sources, top_category, top_country,
             avg_title_length, articles_by_category, articles_by_source, articles_by_country)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                analytics['summary_date'],
                analytics['total_articles'],
                analytics['unique_sources'],
                analytics['top_category'],
                analytics['top_country'],
                analytics['avg_title_length'],
                analytics['articles_by_category'],
                analytics['articles_by_source'],
                analytics['articles_by_country']
            ))
            self.conn.commit()
            return True
        except Exception as e:
            print(f"Error saving summary: {e}")
            return False
    
    def run_analytics(self):
        """Run daily analytics job"""
        print(f"Starting daily analytics for {datetime.utcnow().date()}")
        
        articles = self.get_recent_articles()
        print(f"Found {len(articles)} articles from last 24 hours")
        
        if not articles:
            print("No articles to analyze")
            return
        
        analytics = self.compute_analytics(articles)
        
        if analytics:
            print("\n=== Daily Summary ===")
            print(f"Date: {analytics['summary_date']}")
            print(f"Total Articles: {analytics['total_articles']}")
            print(f"Unique Sources: {analytics['unique_sources']}")
            print(f"Top Category: {analytics['top_category']}")
            print(f"Top Country: {analytics['top_country']}")
            print(f"Avg Title Length: {analytics['avg_title_length']}")
            
            if self.save_summary(analytics):
                print("\nâœ“ Analytics saved to daily_summary table")
            else:
                print("\nâœ— Failed to save analytics")
        
    def close(self):
        self.conn.close()


def run_analytics():
    """Entry point for Airflow"""
    analytics = NewsAnalytics()
    try:
        analytics.run_analytics()
    finally:
        analytics.close()


if __name__ == "__main__":
    run_analytics()
EOF

# Create Airflow DAGs
mkdir -p airflow/dags

cat > airflow/dags/job1_ingestion_dag.py << 'EOF'
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import sys
import os

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

from src.job1_producer import run_producer

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'job1_news_ingestion',
    default_args=default_args,
    description='Continuous news ingestion from NewsData.io to Kafka',
    schedule_interval='@hourly',
    catchup=False,
    tags=['news', 'ingestion', 'kafka'],
)

ingestion_task = PythonOperator(
    task_id='fetch_and_produce_news',
    python_callable=run_producer,
    dag=dag,
)

ingestion_task
EOF

cat > airflow/dags/job2_clean_store_dag.py << 'EOF'
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import sys
import os

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

from src.job2_cleaner import run_cleaner

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'job2_news_cleaning',
    default_args=default_args,
    description='Hourly batch: Read Kafka, clean data, store in SQLite',
    schedule_interval='@hourly',
    catchup=False,
    tags=['news', 'cleaning', 'sqlite'],
)

cleaning_task = PythonOperator(
    task_id='clean_and_store_news',
    python_callable=run_cleaner,
    dag=dag,
)

cleaning_task
EOF

cat > airflow/dags/job3_daily_summary_dag.py << 'EOF'
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import sys
import os

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

from src.job3_analytics import run_analytics

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=10),
}

dag = DAG(
    'job3_daily_analytics',
    default_args=default_args,
    description='Daily analytics: Compute summary from SQLite',
    schedule_interval='@daily',
    catchup=False,
    tags=['news', 'analytics', 'summary'],
)

analytics_task = PythonOperator(
    task_id='compute_daily_summary',
    python_callable=run_analytics,
    dag=dag,
)

analytics_task
EOF

echo "âœ… All Python files created successfully!"
echo ""
echo "Next steps:"
echo "1. Edit config.py and add your NewsData.io API key"
echo "2. Run: chmod +x setup_project.sh"
echo "3. Initialize database: python3 src/db_utils.py"



